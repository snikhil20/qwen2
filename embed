#Trial embedding-working

# Step 1: Install the necessary packages
!pip install torch torchvision transformers
!pip install transformers accelerate
!pip install --upgrade transformers

import torch


!pip install git+https://github.com/huggingface/transformers accelerate

!pip install qwen-vl-utils

import pandas as pd
import json
import requests
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch
import torch.nn as nn
import numpy as np

import gc

gc.collect()

torch.cuda.empty_cache()

# Load the model on the available device(s)
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct", torch_dtype="auto", device_map="auto"
)

# Load the processor for preprocessing
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")

import gc

gc.collect()

torch.cuda.empty_cache()


# # Messages containing a video and text
# messages = [
#     {
#         "role": "user",
#         "content": [
#             {"type": "video", "video": "https://path_to_your_video.mp4"},
#             {"type": "text", "text": "Get the embedding for this video."},
#         ],
#     }
# ]

# # Prepare inputs for inference
# text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
# image_inputs, video_inputs = process_vision_info(messages)

# # If there are no images, set `images=[]`
# inputs = processor(
#     text=[text], images=[], videos=video_inputs, padding=True, return_tensors="pt"
# ).to("cuda")




# Messages containing both images, videos, and text (Example: video and image combined)
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://storage.googleapis.com/the-story-teller/originals/organizations/66a07f9ba8e8ba77305c22fe/platforms/649eccb00ef0f7c7c51a9daa/connectors/66a07fd7c98335bb5da262a8/images/66a0abe8c98335bb5da32626.jpg"},

            {"type": "text", "text": "Get the embedding for these media."},
        ],
    }
]

# Prepare inputs for inference
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt"
).to("cuda")

# Forward pass to obtain outputs
with torch.no_grad():
    outputs = model(**inputs)

# Explore different output attributes
# Check if the outputs include logits, embeddings, or any other relevant tensors
if hasattr(outputs, "logits"):
    embeddings = outputs.logits
elif hasattr(outputs, "encoder_last_hidden_state"):
    embeddings = outputs.encoder_last_hidden_state
elif hasattr(outputs, "pooler_output"):
    embeddings = outputs.pooler_output
else:
    raise ValueError("Unable to extract embeddings. Check model output attributes.")

# Detach the embeddings from the computational graph and move them to CPU if needed
embeddings = embeddings.detach().cpu()

print(embeddings)


# Clear the inputs from memory
del inputs
torch.cuda.empty_cache()
# Remove the batch dimension [1, 46, 152064] -> [46, 152064]
embeddings = embeddings.squeeze(0)

# Apply mean pooling across the 46 tokens to get a single representation
pooled_embeddings = torch.mean(embeddings, dim=0)  # Resulting shape [152064]

# Define a linear layer to project the pooled embedding to 768 dimensions
projection_layer = nn.Linear(152064, 768)

# Project the pooled embeddings to the desired shape [768]
final_embeddings = projection_layer(pooled_embeddings)

# Add the batch dimension back to get [1, 768]
final_embeddings = final_embeddings.unsqueeze(0)

print(final_embeddings.shape)  # torch.Size([1, 768])
print(final_embeddings)

# Assuming 'final_embeddings' is your tensor with shape [1, 768]
# If the tensor is on the GPU, move it to the CPU first
if final_embeddings.is_cuda:
    final_embeddings = final_embeddings.cpu()

# Detach the tensor from the computation graph (if necessary)
final_embeddings = final_embeddings.detach()

# Convert the tensor to a numpy array
numpy_embeddings = final_embeddings.numpy()

print(numpy_embeddings.shape)  # Should print (1, 768)


print(numpy_embeddings)

#emdedding for the sheet

import pandas as pd
import json
import requests
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch
import torch.nn as nn
import numpy as np
import gc

# Load the Excel sheet
excel_file_path = "/content/creatives_data_done.xlsx"
creatives_df = pd.read_excel(excel_file_path)

# Load the JSON file with MediaAssetID to imageURL mapping
json_file_path = "/content/thestoryteller-prod.ad_images.json"
with open(json_file_path, 'r') as f:
    media_asset_mapping = json.load(f)

import gc

gc.collect()

torch.cuda.empty_cache()

# Load the model on the available device(s)
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct", torch_dtype="auto", device_map="auto"
)

# Load the processor for preprocessing
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")

def get_embedding_for_image_url(imageUrl):
    # Prepare the message structure for inference
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": imageUrl},
                {"type": "text", "text": "Get the embedding for this image."},
            ],
        }
    ]

    # Prepare inputs for inference
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)

    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to("cuda")

    # Forward pass to obtain outputs
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract embeddings from model outputs
    if hasattr(outputs, "logits"):
        embeddings = outputs.logits
    elif hasattr(outputs, "encoder_last_hidden_state"):
        embeddings = outputs.encoder_last_hidden_state
    elif hasattr(outputs, "pooler_output"):
        embeddings = outputs.pooler_output
    else:
        raise ValueError("Unable to extract embeddings. Check model output attributes.")

    # Detach the embeddings from the computational graph and move them to CPU
    embeddings = embeddings.detach().cpu()

    # Remove the batch dimension [1, 46, 152064] -> [46, 152064]
    embeddings = embeddings.squeeze(0)

    # Apply mean pooling across the 46 tokens to get a single representation
    pooled_embeddings = torch.mean(embeddings, dim=0)  # Resulting shape [152064]

    # Define a linear layer to project the pooled embedding to 768 dimensions
    projection_layer = nn.Linear(152064, 768)

    # Project the pooled embeddings to the desired shape [768]
    final_embeddings = projection_layer(pooled_embeddings)

    # Add the batch dimension back to get [1, 768]
    final_embeddings = final_embeddings.unsqueeze(0)

    # Convert the tensor to a numpy array
    if final_embeddings.is_cuda:
        final_embeddings = final_embeddings.cpu()

    final_embeddings = final_embeddings.detach().numpy()

    return final_embeddings

# Initialize a list to store the embeddings
embeddings_list = []

# Iterate through each row in the Excel sheet
for idx, row in creatives_df.iterrows():
    # Clear GPU cache and collect garbage before processing each image
    gc.collect()
    torch.cuda.empty_cache()

    media_asset_id = row['mediaAssetId']  # Replace with the actual column name

    # Get the corresponding image URL from the JSON file
    imageUrl = None
    for item in media_asset_mapping: # Iterate through the list of dictionaries
        if str(item.get('adImageId')) == str(media_asset_id): # Check if the '_id' matches the media_asset_id
            imageUrl = item.get('imageUrl') # Get the 'adImage' value if there's a match
            break

    if imageUrl:
        # Get the embedding for the image URL
        try:
            embedding = get_embedding_for_image_url(imageUrl)
            embeddings_list.append(embedding)
        except Exception as e:
            print(f"Error processing {media_asset_id}: {e}")
            embeddings_list.append(None)
    else:
        print(f"No image URL found for {media_asset_id}")
        embeddings_list.append(None)

# Add the embeddings to the DataFrame
creatives_df['qwenCreatives_embedding'] = embeddings_list

# Save the updated DataFrame to a new Excel file
output_excel_file = "/content/updated_creative_data_done.xlsx"
creatives_df.to_excel(output_excel_file, index=False)

